{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ebbec6d-0ff7-4cbe-847a-656777267b85",
   "metadata": {},
   "source": [
    "# Orange CNN (based on Pasquadibisceglie et al., 2020) \n",
    "\n",
    "Adapted from the original code by: \n",
    "\n",
    "- Pasquadibisceglie, V., Appice, A., Castellano, G., Malerba, D., & Modugno, G. (2020). Orange: Outcome-oriented predictive process monitoring based on image encoding and CNNs. IEEE Access, 8, 184073â€“184086. https://doi.org/10.1109/ACCESS.2020.3029323\n",
    "\n",
    "github link: https://github.com/vinspdb/ORANGE\n",
    "\n",
    "This notebook contains the code that tests an Orange CNN model originally created by Pasquadibisceglie et al. on the data provided by the Catharina hospital\n",
    "\n",
    "## General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fa7f55d-ccc7-4ea3-b3d0-6efc8308b997",
   "metadata": {},
   "outputs": [],
   "source": [
    "#general imports\n",
    "import sys\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from glob import glob\n",
    "pd.options.mode.chained_assignment = None  \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#hyperas imports for hyperparameter optimization\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "from tensorflow.compat.v1 import set_random_seed\n",
    "set_random_seed(seed)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, MaxPooling2D, BatchNormalization, Conv2D, Activation, GlobalMaxPooling2D\n",
    "from tensorflow.keras import regularizers, losses\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC, Accuracy, MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "import tensorflow as tf\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import feature_selection\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from other_lib import globalvar\n",
    "from other_lib.auk_score import AUK\n",
    "from other_lib.general_functions import prepare_dataset_for_model, find_all_csv_locations, image_encoder_cv, image_encoder_val\n",
    "from orange_lib import DeepInsight_train_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03fa33f-32cf-445b-afb5-e2ec67695922",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "code use to build the prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18d39425-4a43-4662-9616-aa610fd35dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x_train, x_val, x_test, y_train, y_val, y_test, binary):\n",
    "\n",
    "    #convert input data to image data\n",
    "    x_train_images, x_val_images, x_test_images = image_encoder_val(x_train, x_val, x_test, y_train, y_val, y_test)\n",
    "    \n",
    "    #build sequential model\n",
    "    model = Sequential()\n",
    "    input_shape = (x_train_images.shape[1], x_train_images.shape[1], 1)\n",
    "    model.add(Conv2D({{choice([32, 64])}}, (2, 2), input_shape=input_shape, padding='same', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2({{choice([0.001, 0.0001])}})))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D({{choice([32, 64])}}, (4, 4), padding='same', kernel_initializer='glorot_uniform', kernel_regularizer=regularizers.l2({{choice([0.001, 0.0001])}})))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(GlobalMaxPooling2D())\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate={{choice([10 ** -4, 10 ** -3, 10 ** -2])}}, clipnorm=1.)   \n",
    "\n",
    "    #add output layer based on the prediction type        \n",
    "    if binary:\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy',\n",
    "                      metrics=['accuracy', globalvar.f1, globalvar.precision, globalvar.recall, globalvar.auc])\n",
    "    else:\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "        model.compile(optimizer=optimizer, loss='mae', metrics=['mae', 'mse', 'mape'])\n",
    "        \n",
    "    model.summary()\n",
    "    \n",
    "    earlystop = EarlyStopping(monitor='val_loss', min_delta=0.000001, patience=10, verbose=0, mode='min')\n",
    "    lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, verbose=0, mode='auto',\n",
    "                                       min_delta=0.0001, cooldown=0, min_lr=0)\n",
    "    callbacks_list = [earlystop, lr_reducer]\n",
    "\n",
    "    model.fit(x_train_images, y_train, epochs={{choice([50,100])}}, \n",
    "              batch_size={{choice([50, 100])}}, verbose=0, \n",
    "              callbacks=callbacks_list, \n",
    "              validation_data=(x_val_images, y_val))\n",
    "    \n",
    "    score = model.evaluate(x_test_images, y_test, verbose=0)\n",
    "    print('score evaluated: ', score)\n",
    "    print('binary: ', binary)\n",
    "    \n",
    "    if binary:\n",
    "        f1 = score[2]\n",
    "        return {'loss': -f1, 'status': STATUS_OK, 'model': model} #take the negative of f1 here since objective is to minimize and f1 usually maens higher is better\n",
    "    else:\n",
    "        mae = score[1]\n",
    "        return {'loss': mae, 'status': STATUS_OK, 'model': model} #dont take negative value here since you want to minimize the mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d67d957-9ac4-4c42-8ada-e7aa44c5018a",
   "metadata": {},
   "source": [
    "## With best model, calculate cv scores\n",
    "function below is used to crossvalidate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97924670-8b57-4344-a658-b0cfc3f0a642",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cross_validate_best_model(X, y, best_model, best_run, binary, output_dir, model_name, model_type):\n",
    "    if binary: \n",
    "        kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) #cannot do stratifiedkfold for regression tasks\n",
    "        cv_accuracy_scores = []\n",
    "        cv_f1_scores = []\n",
    "        cv_precision_scores = []\n",
    "        cv_recall_scores = []\n",
    "        cv_auc_scores = []\n",
    "        cv_auk_scores = []\n",
    "    else:\n",
    "        kfold = KFold(n_splits=5, shuffle=True, random_state=42) #regular kfold here\n",
    "        cv_mae_scores = []\n",
    "        cv_mse_scores = []\n",
    "        cv_mape_scores = []\n",
    "\n",
    "    callbacks_list = [globalvar.earlystop]\n",
    "    fold_counter = 1\n",
    "\n",
    "    for train, test in kfold.split(X, y): #cross validation to obtain stable results, only have to do padded_X since padded_X1 has same \n",
    "        print('Now starting fold: {} for model: {}'.format(fold_counter, model_name))\n",
    "        \n",
    "        x_train = X.loc[train]\n",
    "        x_test = X.loc[test]\n",
    "        y_train = y.loc[train]\n",
    "        y_test = y.loc[test]\n",
    "\n",
    "        #fill NaN value with mean of training data for both train and test data. Cant do mean per group since many groups have no data at all\n",
    "        x_train.fillna(x_train.mean(), inplace=True)\n",
    "        x_test.fillna(x_train.mean(), inplace=True)\n",
    "\n",
    "        #scaling for non-additional features, only on train/test data to prevent data leakage, complete X returned without scaling\n",
    "        additional_features = ['MedicationCode_B01AA04', 'MedicationCode_B01AA07', 'MedicationCode_B01AE07', 'MedicationCode_B01AF01', \n",
    "                               'MedicationCode_B01AF02', 'MedicationCode_B01AF03', 'MedicationCode_N02AJ13', 'MedicationCode_N02BE01',\n",
    "                               'PlannedDuration', 'Duration', 'MedicationType', 'NOAC', 'MedicationStatus', 'temperature', \n",
    "                               'bloodPressure', 'Test_Hemoglobine', 'Test_eGFR', 'Test_INR', 'Test_Trombocyten']\n",
    "\n",
    "        scaler = StandardScaler()    \n",
    "\n",
    "        if 'tokenized' in model_name and 'transformer' not in model_type: #means all columns need to be encoded, regardless of additional or not\n",
    "            x_train = pd.DataFrame(scaler.fit_transform(x_train))\n",
    "            x_test = pd.DataFrame(scaler.fit_transform(x_test))\n",
    "        elif 'additional' in model_name.lower() and 'ae_agg' not in model_name.lower(): #means only the additionally added columns need to be scaled\n",
    "            x_train[additional_features] = scaler.fit_transform(x_train[additional_features])\n",
    "            x_test[additional_features] = scaler.fit_transform(x_test[additional_features])\n",
    "\n",
    "        #For lstm models, the input needs to be 3d instead of 2d. Therefore, add another dimension to the data\n",
    "        if model_type == 'lstm' or model_type=='transformer':\n",
    "            x_train = np.expand_dims(x_train, -1)\n",
    "            x_test= np.expand_dims(x_test, -1)\n",
    "        \n",
    "            \n",
    "        #convert input data to image data\n",
    "        x_train_images, x_test_images = image_encoder_cv(x_train, x_test, y_train, y_test)\n",
    "            \n",
    "        best_model.fit(x_train_images,\n",
    "                       y_train, \n",
    "                       epochs=best_run['epochs'], \n",
    "                       callbacks=callbacks_list, \n",
    "                       batch_size=best_run['batch_size'],\n",
    "                       verbose=0)\n",
    "\n",
    "        scores = best_model.evaluate(x_test_images, y_test, verbose=0)\n",
    "\n",
    "        if binary:\n",
    "            y_pred = best_model.predict(x_test_images, verbose=0)\n",
    "            scores.append(AUK(y_test, y_pred.flatten()).calculate_auk()) #add AUK scores\n",
    "            \n",
    "            print(\"%s: %.2f%%\" % (best_model.metrics_names[1], scores[1] * 100)) #accuracy of the test prediction\n",
    "            cv_accuracy_scores.append(scores[1])\n",
    "            cv_f1_scores.append(scores[2])\n",
    "            cv_precision_scores.append(scores[3])\n",
    "            cv_recall_scores.append(scores[4])\n",
    "            cv_auc_scores.append(scores[5])\n",
    "            cv_auk_scores.append(scores[6])\n",
    "        else:\n",
    "            print('{} score: {}'.format(best_model.metrics_names[1], scores[1]))\n",
    "            cv_mae_scores.append(scores[1])\n",
    "            cv_mse_scores.append(scores[2])\n",
    "            cv_mape_scores.append(scores[3])\n",
    "\n",
    "        fold_counter += 1 #update fold counter\n",
    "\n",
    "    #calculate measures\n",
    "    if binary:\n",
    "        print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cv_accuracy_scores)*100, numpy.std(cv_accuracy_scores)*100))\n",
    "        measures = [numpy.mean(cv_accuracy_scores), \n",
    "                    numpy.std(cv_accuracy_scores),\n",
    "                    numpy.mean(cv_f1_scores), \n",
    "                    numpy.std(cv_f1_scores),\n",
    "                    numpy.mean(cv_precision_scores),\n",
    "                    numpy.std(cv_precision_scores),\n",
    "                    numpy.mean(cv_recall_scores), \n",
    "                    numpy.std(cv_recall_scores),\n",
    "                    numpy.mean(cv_auc_scores), \n",
    "                    numpy.std(cv_auc_scores),\n",
    "                    numpy.mean(cv_auk_scores),\n",
    "                    numpy.std(cv_auk_scores)] #average over all splits\n",
    "    else:\n",
    "        print('average mae score over all splits: {} (+/- {}%)'.format(numpy.mean(cv_mae_scores), numpy.std(cv_mae_scores)))\n",
    "        measures = [numpy.mean(cv_mae_scores),\n",
    "                    numpy.std(cv_mae_scores),\n",
    "                    numpy.mean(cv_mse_scores),\n",
    "                    numpy.std(cv_mse_scores),\n",
    "                    numpy.mean(cv_mape_scores),\n",
    "                    numpy.std(cv_mape_scores)]\n",
    "\n",
    "    #save and write results + model\n",
    "    if binary:\n",
    "        numpy.savetxt(output_dir + 'results\\\\' + model_name + '-' + str(numpy.mean(cv_accuracy_scores).round(2)) + '.csv', numpy.atleast_2d(measures),\n",
    "                      delimiter=',', fmt='%6f', header=\"acc, acc_std, f1, f1_std, precision, precision_std, recall, recall_std, auc, auc_std, auk, auk_std\") #write the model scores to a csv file\n",
    "\n",
    "        if model_type == 'transformer':\n",
    "            best_model.save_weights(output_dir + 'models\\\\' + model_name + '_model-weights.h5', save_format='h5') #transformer models can only save weights, not complete models\n",
    "        else:\n",
    "            best_model.save(output_dir + 'models\\\\' + model_name + '.h5')\n",
    "\n",
    "        text_file = open(output_dir + 'results\\\\hyperparameters\\\\' + model_name + \"-\" + str(numpy.mean(cv_accuracy_scores).round(2)) + \".txt\", \"w\") #write hyperparameters of best run\n",
    "        text_file.write(str(best_run))\n",
    "        text_file.close()\n",
    "    else:\n",
    "        numpy.savetxt(output_dir + 'results\\\\' + model_name + '-' + str(numpy.mean(cv_mae_scores).round(2)) + '.csv', numpy.atleast_2d(measures),\n",
    "                      delimiter=',', fmt='%6f', header='mae, mae_std, mse, mse_std, mape, mape_std') #write the model scores to a csv file\n",
    "\n",
    "        if model_type == 'transformer':\n",
    "            best_model.save_weights(output_dir + 'models\\\\' + model_name + '_model-weights.h5', save_format='h5') #transformer models can only save weights, not complete models\n",
    "        else:\n",
    "            best_model.save(output_dir + 'models\\\\' + model_name + '.h5')\n",
    "\n",
    "        text_file = open(output_dir + 'results\\\\hyperparameters\\\\' + model_name + '-' + str(numpy.mean(cv_mae_scores).round(2)) + '.txt', 'w') #write hyperparameters of best run\n",
    "        text_file.write(str(best_run))\n",
    "        text_file.close() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d7159c-a071-43cc-a416-0d5f8883bb63",
   "metadata": {},
   "source": [
    "## Loop for all combinations\n",
    "function below combines all functions into a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dd3b556-25a3-4a98-a806-d68ee6672083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pasqua_et_al(file_location, output_dir):\n",
    "    model_name = file_location.split(\"\\\\\")[-1:][0].split('.')[0] #get filename (without.csv)\n",
    "    print('Now starting with dataset: {}'.format(model_name))\n",
    "\n",
    "    #preprocess and split training/test data, also encode to images with pixel values\n",
    "    x_train, x_val, x_test, y_train, y_val, y_test, binary, X, y, model_type = prepare_dataset_for_model(file_location, model_type='cnn')\n",
    "    x_train_images, x_val_images, x_test_images = image_encoder_val(x_train, x_val, x_test, y_train, y_val, y_test)\n",
    "    \n",
    "    #optimize the model hyperparameters through hyperas \n",
    "    best_run, best_model = optim.minimize(model=create_model,\n",
    "                                  data=prepare_dataset_for_model,\n",
    "                                  algo=tpe.suggest,\n",
    "                                  max_evals=5, #number of \"random\" parameter configurations that are tested\n",
    "                                  trials=Trials(),\n",
    "                                  functions=[image_encoder_val],\n",
    "                                  data_args=(file_location, 'cnn'), #supply the arguments for the prepare_dataset_for_model function here\n",
    "                                  eval_space=True,\n",
    "                                  notebook_name='(Pasquadibisceglie et al., 2020)',\n",
    "                                  verbose=False)\n",
    "    \n",
    "    print(\"Evalutation of best performing model:\")\n",
    "    best_scores = best_model.evaluate(x_test_images, y_test, verbose=0)\n",
    "    print(best_scores)\n",
    "    print(best_model.metrics_names)\n",
    "\n",
    "    print(\"Best performing model chosen hyper-parameters:\")\n",
    "    print(best_run)\n",
    "    \n",
    "    #add AUK & Kappa scores and save the best performing optimized model\n",
    "    if binary:\n",
    "        y_pred = best_model.predict(x_test_images, verbose=0)\n",
    "        best_scores.append(AUK(y_test, y_pred.flatten()).calculate_auk())\n",
    "        best_scores.append(AUK(y_test, y_pred.flatten()).kappa_curve())\n",
    "        pd.DataFrame(best_scores).transpose().to_csv(output_dir + 'opt_results\\\\' + model_name + '.csv')\n",
    "    else:\n",
    "        pd.DataFrame(best_scores).transpose().to_csv(output_dir + 'opt_results\\\\' + model_name + '.csv')\n",
    "\n",
    "    \n",
    "    #cross validate to obtain reliable performance of best performing model\n",
    "    cross_validate_best_model(X=X, y=y, best_model=best_model, best_run=best_run, binary=binary, output_dir=output_dir, model_name=model_name, model_type=model_type)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bdf9d0-c1c7-4b57-b711-98488f8ff7e0",
   "metadata": {},
   "source": [
    "Finally, generate the results using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f6ff086-6300-46a5-916b-866a817ba843",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 csv files left\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'C:\\\\Users\\\\20190337\\\\Downloads\\\\Tracebook_v2 (Projectfolder)\\\\model_results\\\\orange\\\\'\n",
    "file_locations = find_all_csv_locations('orange')\n",
    "\n",
    "for file_location in file_locations:\n",
    "    pasqua_et_al(file_location, output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
