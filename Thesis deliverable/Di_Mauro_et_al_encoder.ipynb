{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e6edf4c-79b8-4169-9faf-87e2c7d9e393",
   "metadata": {},
   "source": [
    "## Inception CNN (based on Di Mauro et al, 2019) - Embedding encoding version\n",
    "\n",
    "Adapted from the original code by: \n",
    "\n",
    "- Di Mauro, N., Appice, A., & Basile, T. M. A. (2019). Activity Prediction of Business Process Instances with Inception CNN Models. Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 11946 LNAI(November), 348â€“361. https://doi.org/10.1007/978-3-030-35166-3_25 \n",
    "\n",
    "github link: https://github.com/TaXxER/rnnalpha\n",
    "\n",
    "This notebook contains the code used to calculate the embedding + time differences encoding. Note that this notebook has only been used to determine to retrieve the embedding + time differences encoding, as described in the original paper by Di Mauro et al. The performance of the Di Mauro model on the other encoding strategies is done through the '(Di Mauro et al. - CNN)' notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2012e88b-8cc1-4024-87ea-e3f1468518e6",
   "metadata": {},
   "source": [
    "## General imports and data preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d3d9407-113e-4d87-85b6-d2fafb0c8d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#general imports\n",
    "import sys\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from glob import glob\n",
    "pd.options.mode.chained_assignment = None\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#hyperas imports for hyperparameter optimization\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "\n",
    "#tensorflow imports for building neural networks\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Concatenate, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D, Reshape, MaxPooling1D, Flatten, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.utils import to_categorical, custom_object_scope\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC, Accuracy, MeanSquaredError, MeanAbsoluteError\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.optimizers\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "#sklearn imports for preprocessing, measuring performance and cross validation\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, normalize, StratifiedKFold, KFold\n",
    "from sklearn import feature_selection\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from other_lib import globalvar\n",
    "from other_lib.auk_score import AUK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce12114-1c3d-41c4-a6c7-2ce02501500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#general data preparation function\n",
    "def prepare_dataset_for_model(file_location, model_type):\n",
    "    \n",
    "    df = pd.read_csv(file_location)\n",
    "    model_name = file_location.split(\"\\\\\")[-1:][0].split('.')[0] #get filename (without.csv)\n",
    "    binary = False if 'los' in model_name.lower() else True #check if a binary prediction (for paracetamol/cancel datasets) or a regression prediction (for length of stay) is being made\n",
    "    \n",
    "    #define label (aka outcome) and prediction data\n",
    "    y = df['Label'] if 'Label' in df else df['outcome']\n",
    "    X = df.loc[:, df.columns != 'Label'] if 'Label' in df else df.loc[:, df.columns != 'outcome']\n",
    "    \n",
    "    #remove TraceID (aka case_id) from the training and testing data\n",
    "    if 'TraceID' in X.columns or 'case_id' in X.columns:\n",
    "        X = X.drop('TraceID', 1) if 'TraceID' in X.columns else X.drop('case_id', 1)\n",
    "    \n",
    "    #train/test set split, must be done before scaling to prevent data leakage between train/test data\n",
    "    if binary:\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True, stratify=y)\n",
    "    else:\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n",
    "        \n",
    "    #fill NaN value with mean of training data for both train and test data. Cant do mean per group since many groups have no data at all\n",
    "    x_train.fillna(x_train.mean(), inplace=True)\n",
    "    x_test.fillna(x_train.mean(), inplace=True)\n",
    "    \n",
    "    #scaling for non-additional features, only on train/test data to prevent data leakage, complete X returned without scaling\n",
    "    additional_features = ['PlannedDuration', 'Duration', 'MedicationType', 'NOAC', 'MedicationStatus', 'temperature', \n",
    "                          'bloodPressure', 'Test_Hemoglobine', 'Test_eGFR', 'Test_INR', 'Test_Trombocyten']\n",
    "\n",
    "    scaler = StandardScaler()    \n",
    "    \n",
    "    if 'tokenized' in model_name and 'transformer' not in model_type: #means all columns need to be encoded, regardless of additional or not\n",
    "        x_train = pd.DataFrame(scaler.fit_transform(x_train))\n",
    "        x_test = pd.DataFrame(scaler.fit_transform(x_test))\n",
    "    elif 'additional' in model_name.lower() and 'ae_agg' not in model_name.lower(): #means only the additionally added columns need to be scaled\n",
    "        x_train[additional_features] = scaler.fit_transform(x_train[additional_features])\n",
    "        x_test[additional_features] = scaler.fit_transform(x_test[additional_features])\n",
    "        \n",
    "    #For lstm models, the input needs to be 3d instead of 2d. Therefore, add another dimension to the data\n",
    "    if model_type == 'lstm' or model_type=='transformer':\n",
    "        x_train = np.expand_dims(x_train, -1)\n",
    "        x_test= np.expand_dims(x_test, -1) \n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, binary, X, y, model_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d098a981-b91e-4600-a76c-151b2266a2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (15023, 14) | x_test shape:  (3756, 14) | X shape:  (18779, 14)\n",
      "y_train shape:  (15023,) | y_test shape:  (3756,) | y shape:  (18779,)\n"
     ]
    }
   ],
   "source": [
    "#pick one of three file locations of the processed data\n",
    "#file_location = 'C:\\\\Users\\\\20190337\\\\Downloads\\\\Tracebook_v2 (Projectfolder)\\\\EventLog_Processed_Cancel.csv'\n",
    "#file_location = 'C:\\\\Users\\\\20190337\\\\Downloads\\\\Tracebook_v2 (Projectfolder)\\\\EventLog_Processed_Paracetamol.csv'\n",
    "file_location = 'C:\\\\Users\\\\20190337\\\\Downloads\\\\Tracebook_v2 (Projectfolder)\\\\EventLog_Processed_LOS.csv'\n",
    "\n",
    "output_dir = 'C:\\\\Users\\\\20190337\\\\Downloads\\\\Tracebook_v2 (Projectfolder)\\\\model_results\\\\di_mauro_cnn\\\\'\n",
    "model_name = file_location.split(\"\\\\\")[-1:][0].split('.')[0] #get filename (without.csv)\n",
    "\n",
    "x_train, x_test, y_train, y_test, binary, X, y, model_type = prepare_dataset_for_model(file_location, model_type='cnn')\n",
    "\n",
    "print('x_train shape: ', x_train.shape, '| x_test shape: ', x_test.shape, '| X shape: ', X.shape)\n",
    "print('y_train shape: ', y_train.shape, '| y_test shape: ', y_test.shape, '| y shape: ', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42fbeb93-6cae-4bc0-9d40-7da06837453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#further data preparation function that is used to calculate the time differences\n",
    "def prepare_data(file_location):\n",
    "\n",
    "    vocabulary = set()\n",
    "\n",
    "    logreader = pd.read_csv(file_location)\n",
    "    logreader.sort_values(['TraceID', 'Timestamp'], axis=0, ascending=True, inplace=True)\n",
    "    logreader.reset_index().drop(columns='index')\n",
    "    \n",
    "    lastcase = '' \n",
    "    casestarttime = None\n",
    "    lasteventtime = None\n",
    "    firstLine = True\n",
    "\n",
    "    lines = [] #these are all the activity sequences\n",
    "    timeseqs = [] #time sequences (differences between two events: current and previous event)\n",
    "\n",
    "    numcases = 0\n",
    "    max_length = 0\n",
    "\n",
    "    #loop that calculates the time difference between activities\n",
    "    for index, row in logreader.iterrows():\n",
    "        t = datetime.strptime(row[1], \"%Y-%m-%d %H:%M:%S\") #timestamp column\n",
    "        if row[0]!=lastcase:  #'lastcase' is to save the last executed case for the loop\n",
    "            casestarttime = t\n",
    "            lasteventtime = t\n",
    "            lastcase = row[0]\n",
    "            if not firstLine:\n",
    "                lines.append(line)\n",
    "                timeseqs.append(times)\n",
    "                if len(line) > max_length:\n",
    "                    max_length = len(line)\n",
    "            line = []\n",
    "            times = []\n",
    "            numcases += 1\n",
    "\n",
    "        vocabulary.add(row[2])\n",
    "        line.append(row[2])\n",
    "        timesincelastevent = t - lasteventtime\n",
    "        timediff = 86400 * timesincelastevent.days + timesincelastevent.seconds + timesincelastevent.microseconds/1000000\n",
    "        times.append(timediff+1) # +1 to avoid zero values\n",
    "        lasteventtime = t\n",
    "        firstLine = False\n",
    "        \n",
    "    lines.append(line)\n",
    "    timeseqs.append(times)\n",
    "\n",
    "    vocabulary = {key: idx for idx, key in enumerate(vocabulary)}\n",
    "\n",
    "    divisor = np.mean([item for sublist in timeseqs for item in sublist]) #average time between events\n",
    "    print(\"Num cases: \", numcases)\n",
    "    elems_per_fold = int(round(numcases/3))\n",
    "\n",
    "    if len(line) > max_length:\n",
    "        max_length = len(line)\n",
    "\n",
    "    X = []\n",
    "    X1 = []\n",
    "    y = [] #list for outcome variables, either binary values or LoS values\n",
    "\n",
    "    max_length = 0\n",
    "    prefix_sizes = []\n",
    "    seqs = 0\n",
    "    vocab = set() #set so all unique values are stored to measure the vocab size\n",
    "    for seq, time in zip(lines, timeseqs): #lines = acitivies, timeseqs = time differences\n",
    "        \n",
    "        code = [] #list with tokenized activities\n",
    "        code1 = [] #list with log time differences\n",
    "        \n",
    "        for i in range(0,len(seq)):\n",
    "            code.append(vocabulary[seq[i]])\n",
    "            code1.append(np.log(time[i]+1))\n",
    "            vocab.add(seq[i])\n",
    "        \n",
    "        #store size of the prefix\n",
    "        prefix_sizes.append(len(code))\n",
    "        \n",
    "        #update max trace length\n",
    "        if len(code)>max_length:\n",
    "            max_length = len(code)\n",
    "            \n",
    "        X.append(code[:])\n",
    "        X1.append(code1[:])\n",
    "\n",
    "    prefix_sizes = np.array(prefix_sizes)\n",
    "\n",
    "    print(\"Activities: \",vocab )\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_size = (vocab_size + 1 ) // 2\n",
    "\n",
    "    for label in logreader[['TraceID', 'Label']].groupby('TraceID').max()['Label']:\n",
    "        y.append(label)\n",
    "    \n",
    "    X = np.array(X, dtype=object)\n",
    "    X1 = np.array(X1, dtype=object)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # padding\n",
    "    padded_X = pad_sequences(X, maxlen=max_length, padding='pre', dtype='float64')\n",
    "    padded_X1 = pad_sequences(X1, maxlen=max_length, padding='pre', dtype='float64')\n",
    "    \n",
    "    padded_X1 = padded_X1 / np.max(padded_X1) # normalizing time diff values\n",
    "\n",
    "    padded_X_train, padded_X_test, y_train, y_test = train_test_split(padded_X, y, test_size=0.2, random_state=42)\n",
    "    padded_X1_train, padded_X1_test, _, _ = train_test_split(padded_X1, y, test_size=0.2, random_state=42) #can ignore y_train/y_test vals here since same random state\n",
    "\n",
    "    model_name = file_location.split(\"\\\\\")[-1:][0].split('.')[0] #get filename (without.csv)\n",
    "\n",
    "    #check if a binary prediction (for paracetamol/cancel datasets) or a regression prediction (for length of stay) is being made\n",
    "    binary = False if 'los' in model_name.lower() else True\n",
    "    pre_encoded = False\n",
    "    \n",
    "    input_length = max_length\n",
    "    return padded_X_train, padded_X1_train, padded_X_test, padded_X1_test, y_train, y_test, input_length, vocab_size, embedding_size, binary, pre_encoded, padded_X, padded_X1, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec562ee1-0c19-4770-b22f-11344e013ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num cases:  1150\n",
      "Activities:  {'End_Operation', 'Prepare', 'Scheduled', 'Leave_Cathlab', 'Paracetamol', 'Discharge', 'Waitfor_Schedule', 'MeasureBPs', 'Test_Trombocyten', 'MeasureTemps', 'Arrive_Cathlab', 'Stop_AC', 'Recovery', 'Start_Operation', 'Test_INR', 'Admission', 'Test_Hemoglobine', 'Cancellation', 'Restart_NOAC', 'Test_eGFR', 'End_Introduction', 'Start_AC', 'Start_Introduction'}\n"
     ]
    }
   ],
   "source": [
    "#file_location='C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/EventLog_Processed_Cancel.csv'\n",
    "#file_location='C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/EventLog_Processed_Paracetamol.csv'\n",
    "file_location='C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/EventLog_Processed_LOS.csv'\n",
    "\n",
    "\n",
    "padded_X_train, padded_X1_train, padded_X_test, padded_X1_test, y_train, y_test, input_length, vocab_size, embedding_size, binary, pre_encoded, padded_X, padded_X1, y = prepare_data(file_location)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f71a91-680b-49ae-8b51-1c3daa467a1f",
   "metadata": {},
   "source": [
    "## Building the model\n",
    "code use to build the prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb980749-1e23-434d-bb88-45a8d83e4d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that calculates the embeddings for each activity for each trace\n",
    "def create_model(padded_X_train, padded_X1_train, padded_X_test, padded_X1_test, y_train, y_test,\n",
    "              input_length=input_length, vocab_size=vocab_size, embedding_size=embedding_size,\n",
    "              binary=binary, pre_encoded=pre_encoded):\n",
    "    \n",
    "    inputs = [] #create empty list to store the input layers in, will be updated and used when compiling the model\n",
    "    \n",
    "    if not pre_encoded: #if the model needs to calculate the encoding itself (tokenized activities + time differences)\n",
    "        activity_input = Input(shape=(input_length)) #input layer for activity tokens\n",
    "        activity_embedding = Embedding(input_dim=vocab_size, \n",
    "                                       output_dim=embedding_size, \n",
    "                                       input_length=input_length)(activity_input) #embedding layer for activity tokens\n",
    "\n",
    "        timedif_input = Input(shape=(input_length)) #input layer for time differences\n",
    "        timedif_reshape = Reshape((input_length, 1))(timedif_input) #reshape layer for time differences\n",
    "\n",
    "        concat_layer = Concatenate(axis=2)([activity_embedding, timedif_reshape]) #concatenate the timedif and encoding values to finish the encoding\n",
    "        inputs = [activity_input, timedif_input] #store input layers\n",
    "    else: #if an pre-existing encoded dataset is used, such as the aggregated or regular tokenized datasets\n",
    "        input_layer = Input(shape=(input_length))\n",
    "        inputs = [input_layer]\n",
    "        pass\n",
    "    \n",
    "    #After this point the model creation continues past the encoded data stage\n",
    "    for m in range({{choice([1,2,3])}}): #number of inception modules you want\n",
    "        filters = []\n",
    "        for i in range(3):\n",
    "            filters.append(Conv1D(filters=32, strides=1, kernel_size=1+i, activation='relu', padding='same')(concat_layer)) #add the conv layers of different sizes\n",
    "        filters.append(MaxPooling1D(pool_size=3, strides=1, padding='same')(concat_layer)) #add the max pool layer\n",
    "        concat_layer = Concatenate(axis=2)(filters) #concatenate the output of the different conv modules and max pool layer to get output of inception module\n",
    "\n",
    "    pool = GlobalMaxPooling1D()(concat_layer)\n",
    "    \n",
    "    choiceval = {{choice(['adam', 'sgd', 'rmsprop'])}}\n",
    "    if choiceval == 'adam':\n",
    "        optim = tensorflow.keras.optimizers.Adam(learning_rate={{choice([10 ** -4, 10 ** -3, 10 ** -2])}}, clipnorm=1.)\n",
    "    elif choiceval == 'rmsprop':\n",
    "        optim = rmsprop = tensorflow.keras.optimizers.RMSprop(learning_rate={{choice([10 ** -4, 10 ** -3, 10 ** -2])}}, clipnorm=1.)\n",
    "    else:\n",
    "        optim = tensorflow.keras.optimizers.SGD(learning_rate={{choice([10 ** -4, 10 ** -3, 10 ** -2])}}, clipnorm=1.)\n",
    "    \n",
    "    #determine output shape based on prediction task, either for binary/length of stay prediction\n",
    "    if binary:\n",
    "        output_layer = Dense(1, activation='sigmoid')(pool)\n",
    "        model = Model(inputs=inputs, outputs=output_layer)\n",
    "        model.compile(optimizer=optim, loss='binary_crossentropy',\n",
    "                      metrics=['accuracy', globalvar.f1, globalvar.precision, globalvar.recall, globalvar.auc])\n",
    "        print('Created binary model!')\n",
    "    else:\n",
    "        output_layer = Dense(1, activation='linear')(pool)\n",
    "        model = Model(inputs=inputs, outputs=output_layer)\n",
    "        model.compile(optimizer=optim, loss='mean_absolute_error', \n",
    "                      metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "    \n",
    "    earlystop = EarlyStopping(monitor='val_loss', min_delta=0.000001, patience=15, verbose=1, mode='min')\n",
    "    callbacks_list = [earlystop]\n",
    "    \n",
    "    model.summary()\n",
    "    \n",
    "    model.fit([padded_X_train, padded_X1_train],\n",
    "               y_train, \n",
    "               epochs={{choice([50, 100])}}, \n",
    "               validation_split=0.2, \n",
    "               callbacks=callbacks_list, \n",
    "               batch_size={{choice([2**9, 2**10])}})\n",
    "    \n",
    "    score = model.evaluate([padded_X_test, padded_X1_test], y_test, verbose=0)\n",
    "\n",
    "    if binary:\n",
    "        accuracy = score[1]\n",
    "        return {'loss': -accuracy, 'status': STATUS_OK, 'model': model} #take the negative of accuracy here since objective is to minimize and accuracy usually maens higher is better\n",
    "    else:\n",
    "        mae = score[1]\n",
    "        return {'loss': mae, 'status': STATUS_OK, 'model': model} #dont take negative value here since you want to minimize the mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a48045-968b-4616-ade7-b76ec6dde30b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#file_location = 'C:\\\\Users\\\\20190337\\\\Downloads\\\\Tracebook_v2 (Projectfolder)\\\\EventLog_Processed_Cancel.csv'\n",
    "#file_location = 'C:\\\\Users\\\\20190337\\\\Downloads\\\\Tracebook_v2 (Projectfolder)\\\\EventLog_Processed_Paracetamol.csv'\n",
    "file_location = 'C:\\\\Users\\\\20190337\\\\Downloads\\\\Tracebook_v2 (Projectfolder)\\\\EventLog_Processed_LOS.csv'\n",
    "\n",
    "best_run, best_model = optim.minimize(model=create_model,\n",
    "                                  data=prepare_data,\n",
    "                                  algo=tpe.suggest,\n",
    "                                  max_evals=5, #number of \"random\" parameter configurations that are tested\n",
    "                                  trials=Trials(),\n",
    "                                  data_args=(file_location,), #supply the arguments for the prepare_dataset_for_model function here\n",
    "                                  eval_space=True,\n",
    "                                  notebook_name='Di_Mauro_et_al_encoder',\n",
    "                                  verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb31da6-46d3-4cd5-bd28-c3f08e4d8b9b",
   "metadata": {},
   "source": [
    "### With best model, calculate cross validated scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2866381-5c41-4c4a-8180-5c3e2e3b8e69",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#choose one of three prediction targets\n",
    "#file_location = 'C:\\\\Users\\\\20190337\\\\Downloads\\\\Tracebook_v2 (Projectfolder)\\\\EventLog_Processed_Cancel.csv'\n",
    "#file_location = 'C:\\\\Users\\\\20190337\\\\Downloads\\\\Tracebook_v2 (Projectfolder)\\\\EventLog_Processed_Paracetamol.csv'\n",
    "file_location = 'C:\\\\Users\\\\20190337\\\\Downloads\\\\Tracebook_v2 (Projectfolder)\\\\EventLog_Processed_LOS.csv'\n",
    "\n",
    "model_name = file_location.split(\"\\\\\")[-1:][0].split('.')[0] #get filename (without.csv)\n",
    "padded_X_train, padded_X1_train, padded_X_test, padded_X1_test, y_train, y_test, input_length, vocab_size, embedding_size, binary, pre_encoded, padded_X, padded_X1, y = prepare_data(file_location)\n",
    "\n",
    "if binary: \n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42) #cannot do stratifiedkfold for regression tasks\n",
    "    cv_accuracy_scores = []\n",
    "    cv_f1_scores = []\n",
    "    cv_precision_scores = []\n",
    "    cv_recall_scores = []\n",
    "    cv_auc_scores = []\n",
    "else:\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42) #regular kfold here\n",
    "    cv_mae_scores = []\n",
    "    cv_mse_scores = []\n",
    "\n",
    "callbacks_list = [globalvar.earlystop]\n",
    "\n",
    "for train, test in kfold.split(padded_X, y): #cross validation to obtain stable results, only have to do padded_X since padded_X1 has same indexes\n",
    "    best_model.fit([padded_X[train], padded_X1[train]], #use the same [train] indexes for both padded_X and padded_X1 to get correct values\n",
    "                   y[train],\n",
    "                   epochs=best_run['epochs'], \n",
    "                   callbacks=callbacks_list, \n",
    "                   batch_size=best_run['batch_size'],\n",
    "                   verbose=0)\n",
    "\n",
    "    scores = best_model.evaluate([padded_X[test], padded_X1[test]], y[test], verbose=0)\n",
    "    \n",
    "    if binary:\n",
    "        print(\"%s: %.2f%%\" % (best_model.metrics_names[1], scores[1] * 100)) #accuracy of te test prediction\n",
    "        cv_accuracy_scores.append(scores[1])\n",
    "        cv_f1_scores.append(scores[2])\n",
    "        cv_precision_scores.append(scores[3])\n",
    "        cv_recall_scores.append(scores[4])\n",
    "        cv_auc_scores.append(scores[5])\n",
    "    else:\n",
    "        print('{} score: {}'.format(best_model.metrics_names[1], scores[1]))\n",
    "        cv_mae_scores.append(scores[1])\n",
    "        cv_mse_scores.append(scores[2])\n",
    "\n",
    "if binary:\n",
    "    print(\"%.2f%% (+/- %.2f%%)\" % (numpy.mean(cv_accuracy_scores)*100, numpy.std(cv_accuracy_scores)*100))\n",
    "    measures = [numpy.mean(cv_accuracy_scores), numpy.mean(cv_f1_scores), numpy.mean(cv_precision_scores),\n",
    "                numpy.mean(cv_recall_scores), numpy.mean(cv_auc_scores)] #average over all splits\n",
    "else:\n",
    "    print('average mae score over all splits: {} (+/- {}%)'.format(numpy.mean(cv_mae_scores), numpy.std(cv_mae_scores)))\n",
    "    measures = [numpy.mean(cv_mae_scores), numpy.mean(cv_mse_scores)]\n",
    "\n",
    "#save results\n",
    "output_dir = 'C:\\\\Users\\\\20190337\\\\Downloads\\\\Tracebook_v2 (Projectfolder)\\\\model_results\\\\di_mauro_cnn\\\\'\n",
    "model_name = file_location.split(\"\\\\\")[-1:][0].split('.')[0] #get filename (without.csv)\n",
    "\n",
    "if binary:\n",
    "    numpy.savetxt(output_dir + 'results\\\\' + model_name + \"-\" + str(numpy.mean(cv_accuracy_scores).round(2)) + \".csv\", numpy.atleast_2d(measures),\n",
    "                  delimiter=',', fmt='%6f', header=\"acc, f1, precision, recall, auc\") #write the model scores to a csv file\n",
    "\n",
    "    best_model.save(output_dir + 'models\\\\' + model_name + '.h5')\n",
    "\n",
    "    text_file = open(output_dir + 'results\\\\hyperparameters\\\\' + model_name + \"-\" + str(numpy.mean(cv_accuracy_scores).round(2)) + \".txt\", \"w\") #write hyperparameters of best run\n",
    "    text_file.write(str(best_run))\n",
    "    text_file.close()\n",
    "else:\n",
    "    numpy.savetxt(output_dir + 'results\\\\' + model_name + \"-\" + str(numpy.mean(cv_mae_scores).round(2)) + \".csv\", numpy.atleast_2d(measures),\n",
    "                  delimiter=',', fmt='%6f', header=\"mae, mse\") #write the model scores to a csv file\n",
    "\n",
    "    best_model.save(output_dir + 'models\\\\' + model_name + '.h5')\n",
    "\n",
    "    text_file = open(output_dir + 'results\\\\hyperparameters\\\\' + model_name + \"-\" + str(numpy.mean(cv_mae_scores).round(2)) + \".txt\", \"w\") #write hyperparameters of best run\n",
    "    text_file.write(str(best_run))\n",
    "    text_file.close()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3511226c-b8e9-414a-b9a0-bc3b7f42d5de",
   "metadata": {},
   "source": [
    "## Retrieve embedding + timediff encoding\n",
    "\n",
    "Take and store output from the embedding+timedif layers to use as encoded input for other models using the best scoring prediction model determine above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af00742a-8dbd-42f5-a1f8-7bfb118232c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that calculates the average embeddings for each trace \n",
    "def average_feature_vector(embeddings):\n",
    "    avg_embedding = []\n",
    "    for trace in embeddings: # for each trace\n",
    "        avg_vector = []\n",
    "        for activity in trace: # for each activity in the trace\n",
    "            avg_vector.append(activity.mean())\n",
    "        \n",
    "        avg_embedding.append(np.array(avg_vector)) #store the avg value per activity per trace\n",
    "    return avg_embedding\n",
    "\n",
    "#function for saving the embedded traces\n",
    "def export_and_save_traces(df, filename, folder_path = 'C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/'):\n",
    "    \n",
    "    #Save preprocessed data to new files, check if the file already exists or not\n",
    "    if os.path.exists(folder_path + filename + '.csv'):\n",
    "        overwrite = input('Warning, files already exist. Do you want to overwrite? type y/n: ') \n",
    "        \n",
    "        if overwrite.lower() == 'y': #check if you want to overwrite the files if they already exist\n",
    "            df.to_csv(path_or_buf = folder_path + filename + '.csv', sep=',', index=False)\n",
    "            df.to_excel(excel_writer = folder_path + filename + '.xlsx', index=False)\n",
    "            print('Files succesfully overwritten')\n",
    "        else:\n",
    "            print('Files not overwritten.')\n",
    "    else:\n",
    "        df.to_csv(path_or_buf = folder_path + filename + '.csv', sep=',', index=False)\n",
    "        df.to_excel(excel_writer = folder_path + filename + '.xlsx', index=False)\n",
    "        print('New preprocessed logs created: ', filename)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93074fb6-a1ff-4d70-831f-e5ffc4a5c0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define custom metrics so the model can be loaded\n",
    "custom_objects = {'f1': globalvar.f1, 'precision': globalvar.precision, 'recall': globalvar.recall, 'auc': globalvar.auc} #define custom metrics\n",
    "\n",
    "#load the best_model from the .h5 file\n",
    "with custom_object_scope(custom_objects):\n",
    "    best_model_can = load_model('C:\\\\Users\\\\20190337\\\\Downloads\\\\Tracebook_v2 (Projectfolder)\\\\model_results\\\\di_mauro_cnn\\\\models\\\\EventLog_Processed_Cancel.h5')\n",
    "    best_model_par = load_model('C:\\\\Users\\\\20190337\\\\Downloads\\\\Tracebook_v2 (Projectfolder)\\\\model_results\\\\di_mauro_cnn\\\\models\\\\EventLog_Processed_Paracetamol.h5')\n",
    "    best_model_los = load_model('C:\\\\Users\\\\20190337\\\\Downloads\\\\Tracebook_v2 (Projectfolder)\\\\model_results\\\\di_mauro_cnn\\\\models\\\\EventLog_Processed_LOS.h5')\n",
    "\n",
    "#get only the model to the part where the concatenation layer is located, might have to use a different concatenate layer name here\n",
    "can_embedding_model = Model(best_model_can.input,best_model_can.get_layer('concatenate_26').output)\n",
    "par_embedding_model = Model(best_model_par.input,best_model_par.get_layer('concatenate_41').output)\n",
    "los_embedding_model = Model(best_model_los.input,best_model_los.get_layer('concatenate_12').output)\n",
    "\n",
    "#cancelation datasets:\n",
    "file_location = 'C:\\\\Users\\\\20190337\\\\Downloads\\\\Tracebook_v2 (Projectfolder)\\\\EventLog_Processed_Cancel.csv'\n",
    "padded_X_train, padded_X1_train, padded_X_test, padded_X1_test, y_train, y_test, input_length, vocab_size, embedding_size, binary, pre_encoded, padded_X, padded_X1, y = prepare_data(file_location)\n",
    "encoded_can = can_embedding_model.predict([padded_X, padded_X1]) #make prediction to generate embedding\n",
    "avg_encoded_can = average_feature_vector(encoded_can)#calculate the average over the embedding dimension\n",
    "avg_encoded_can_df = pd.DataFrame(avg_encoded_can).add_prefix('feature_') #make a dataframe of embedding\n",
    "avg_encoded_can_df['Label'] = y\n",
    "\n",
    "#paracetamol datasets:\n",
    "file_location = 'C:\\\\Users\\\\20190337\\\\Downloads\\\\Tracebook_v2 (Projectfolder)\\\\EventLog_Processed_Paracetamol.csv'\n",
    "padded_X_train, padded_X1_train, padded_X_test, padded_X1_test, y_train, y_test, input_length, vocab_size, embedding_size, binary, pre_encoded, padded_X, padded_X1, y = prepare_data(file_location)\n",
    "encoded_par = par_embedding_model.predict([padded_X, padded_X1]) #make prediction to generate embedding\n",
    "avg_encoded_par = average_feature_vector(encoded_par)#calculate the average over the embedding dimension\n",
    "avg_encoded_par_df = pd.DataFrame(avg_encoded_par).add_prefix('feature_') #make a dataframe of embedding\n",
    "avg_encoded_par_df['Label'] = y #add label\n",
    "\n",
    "#los datasets:\n",
    "file_location = 'C:\\\\Users\\\\20190337\\\\Downloads\\\\Tracebook_v2 (Projectfolder)\\\\EventLog_Processed_LOS.csv'\n",
    "padded_X_train, padded_X1_train, padded_X_test, padded_X1_test, y_train, y_test, input_length, vocab_size, embedding_size, binary, pre_encoded, padded_X, padded_X1, y = prepare_data(file_location)\n",
    "encoded_los = los_embedding_model.predict([padded_X, padded_X1]) #make prediction to generate embedding\n",
    "avg_encoded_los = average_feature_vector(encoded_los) #calculate the average over the embedding dimension\n",
    "avg_encoded_los_df = pd.DataFrame(avg_encoded_los).add_prefix('feature_') #make a dataframe of embedding\n",
    "avg_encoded_los_df['Label'] = y #add label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd96745-a00b-473f-8cb8-058233e4e63f",
   "metadata": {},
   "source": [
    "\n",
    "Finally, store the embedding + time differences encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e2042eb-a5ae-4975-bd09-086476f085e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Warning, files already exist. Do you want to overwrite? type y/n:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files succesfully overwritten\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Warning, files already exist. Do you want to overwrite? type y/n:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files succesfully overwritten\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Warning, files already exist. Do you want to overwrite? type y/n:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files succesfully overwritten\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Warning, files already exist. Do you want to overwrite? type y/n:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files succesfully overwritten\n"
     ]
    }
   ],
   "source": [
    "folder_path_avg = 'C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/encoded_logs/tokenized_timedif/avg/'\n",
    "\n",
    "#save cancellation encoding:\n",
    "additional_data_df_can = pd.read_csv('C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/encoded_logs/additional_data/additional_can.csv').drop('Label', axis=1) #load the data to add additional features later\n",
    "avg_encoded_can_df_additional = pd.concat([avg_encoded_can_df, additional_data_df_can], axis=1) #axis = 1 since we want to paste columns next to each other instead of rows\n",
    "export_and_save_traces(avg_encoded_can_df, filename='embed_timedif_can_avg', folder_path=folder_path_avg) #save results without additional features\n",
    "export_and_save_traces(avg_encoded_can_df_additional, filename='embed_timedif_can_avg_additional', folder_path=folder_path_avg) #save results with additional features\n",
    "\n",
    "#save paracetamol encoding\n",
    "additional_data_df_par = pd.read_csv('C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/encoded_logs/additional_data/additional_par.csv').drop('Label', axis=1) #load the data to add additional features later\n",
    "avg_encoded_par_df_additional = pd.concat([avg_encoded_par_df, additional_data_df_par], axis=1) #axis = 1 since we want to paste columns next to each other instead of rows                            \n",
    "export_and_save_traces(avg_encoded_par_df, filename='embed_timedif_par_avg', folder_path=folder_path_avg) #save results without additional features\n",
    "export_and_save_traces(avg_encoded_par_df_additional, filename='embed_timedif_par_avg_additional', folder_path=folder_path_avg) #save results with additional features\n",
    "\n",
    "#save los encoding\n",
    "additional_data_df_los = pd.read_csv('C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/encoded_logs/additional_data/additional_los.csv').drop('Label', axis=1) #load the data to add additional features later\n",
    "avg_encoded_los_df_additional = pd.concat([avg_encoded_los_df, additional_data_df_los], axis=1) #axis = 1 since we want to paste columns next to each other instead of rows                            \n",
    "export_and_save_traces(avg_encoded_los_df, filename='embed_timedif_los_avg', folder_path=folder_path_avg) #save results without additional features\n",
    "export_and_save_traces(avg_encoded_los_df_additional, filename='embed_timedif_los_avg_additional', folder_path=folder_path_avg) #save results with additional features\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
