{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "998c8fbb-eff6-46e9-88b4-47afa093c857",
   "metadata": {},
   "source": [
    "# Preprocessing script\n",
    "\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e46ada43-6e59-4a93-85b6-cee0bde87701",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "from multiprocessing import  Pool\n",
    "import csv\n",
    "from time import time\n",
    "from glob import glob\n",
    "\n",
    "#for making embedding matrix\n",
    "import io\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, TextVectorization\n",
    "from gensim.models import Word2Vec\n",
    "from skmultiflow.utils import calculate_object_size\n",
    "from tqdm import tqdm\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.objects.log.util import dataframe_utils\n",
    "from pm4py.objects.log.exporter.xes import exporter as xes_exporter\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.util import constants\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from pandas.api.types import is_string_dtype, is_numeric_dtype\n",
    "\n",
    "#For processtransformer preprocessing steps\n",
    "from transformer_lib import constants\n",
    "from transformer_lib.models import transformer\n",
    "from transformer_lib.data.loader import LogsDataLoader\n",
    "from transformer_lib.data.processor import LogsDataProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ccac2f6e-0869-48a6-adc0-6ae3019761b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional installs, maybe needed if errors occur\n",
    "#!pip install os\n",
    "#!pip install gensim\n",
    "#!pip install scikit-multiflow\n",
    "#!pip install pm4py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f768aee-7750-4510-8552-2dd2684247fc",
   "metadata": {},
   "source": [
    "## Preparing, cleaning and labeling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "edbfb621-186a-4afe-acbf-50fb72cefeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(file):    \n",
    "    df = pd.read_excel(file)\n",
    "\n",
    "    nrows = df.shape[0] #total number of rows\n",
    "    ncols = df.shape[1] #total number of columns\n",
    "    \n",
    "    #Rename a number of columns to fix typos\n",
    "    df = df.rename(columns={'EvantName': 'EventName',\n",
    "                            'PlanedDuration': 'PlannedDuration'})\n",
    "    \n",
    "    #Remove traces with less than 3 activities\n",
    "    df = df[df['TraceID'].map(df['TraceID'].value_counts()) > 2]\n",
    "    \n",
    "    #NOAC to binary\n",
    "    df.loc[df['NOAC'] == 'T', 'NOAC'] = 1\n",
    "    df.loc[df['NOAC'] == 'F', 'NOAC'] = 0\n",
    "    \n",
    "    #MedicationStatus to binary\n",
    "    df.loc[df['MedicationStatus'] == 'continue', 'MedicationStatus'] = 1\n",
    "    df.loc[df['MedicationStatus'] == 'stop', 'MedicationStatus'] = 0\n",
    "    \n",
    "    #MedicationType to binary\n",
    "    df.loc[df['MedicationType'] == 'continue', 'MedicationType'] = 1\n",
    "    df.loc[df['MedicationType'] == 'stop', 'MedicationType'] = 0\n",
    "\n",
    "    #Change datatypes\n",
    "    df['NOAC'] = df['NOAC'].apply(float)\n",
    "    df['MedicationStatus'] = df['MedicationStatus'].apply(float)\n",
    "    df['MedicationType'] = df['MedicationType'].apply(float)\n",
    "    \n",
    "    #one hot encoding for different testing types, and remove the testValue column\n",
    "    df['Test_Hemoglobine'] = np.where(df['EventName'] == 'Test_Hemoglobine', df['testValue'], 0)\n",
    "    df['Test_eGFR'] = np.where(df['EventName'] == 'Test_eGFR', df['testValue'], 0)\n",
    "    df['Test_INR'] = np.where(df['EventName'] == 'Test_INR', df['testValue'], 0)\n",
    "    df['Test_Trombocyten'] = np.where(df['EventName'] == 'Test_Trombocyten', df['testValue'], 0)\n",
    "    df = df.drop(columns = ['testValue'], axis=1)\n",
    "    \n",
    "    #fix the broken temperature values, removing scientific notation and putting the decimal at the correct place in the values\n",
    "    new_temp_vals = [x for x in df[df['temperature']>1]['temperature'].astype(str)] #save the messed up temperature values\n",
    "    new_temp_vals = [''.join(c for c in value if c.isdigit()) for value in new_temp_vals] #remove the '.' that is often located at the wrong spot and keep just the numbers\n",
    "    new_temp_vals = [round(float(''.join(value[:2] + '.' + value[2:])), 2) for value in new_temp_vals] #place a '.' after every 2 numbers, and convert to number to get the correct temperature values\n",
    "    df.loc[df['temperature']>1, 'temperature'] = new_temp_vals\n",
    "    \n",
    "    #replace the 0 (measuring errors) values located in the temperature with the mean, as well as a single huge outlier\n",
    "    df['temperature'] = df['temperature'].replace(0, round(df['temperature'].mean(), 2))\n",
    "    df['temperature'] = df['temperature'].replace(27.12, round(df['temperature'].mean(), 2))\n",
    "    \n",
    "    #also replace 1 very significant outlier in the duration column with the mean value, further outliers are fixed later in code\n",
    "    df['Duration'] = df['Duration'].replace(1588, round(df['Duration'].mean(), 0))\n",
    "    \n",
    "    # fix the broken test_hemoglobine and test_INRvalues\n",
    "    new_hemo_vals = [x for x in df[df['Test_Hemoglobine']>0]['Test_Hemoglobine'].astype(str)] #save the messed up hemoglobine values\n",
    "    new_hemo_vals = [''.join(c for c in value if c.isdigit()) for value in new_hemo_vals] #remove the '.' that is often located at the wrong spot and keep just the numbers\n",
    "    new_hemo_vals = [round(float(''.join(value[:1] + '.' + value[1:])), 2) for value in new_hemo_vals] #place a '.' after every 2 numbers, and convert to number to get the correct hemoglobine values\n",
    "    df.loc[df['Test_Hemoglobine']>0, 'Test_Hemoglobine'] = new_hemo_vals\n",
    "    \n",
    "    new_inr_vals = [x for x in df[df['Test_INR']>0]['Test_INR'].astype(str)] #save the messed up inr values\n",
    "    new_inr_vals = [''.join(c for c in value if c.isdigit()) for value in new_inr_vals] #remove the '.' that is often located at the wrong spot and keep just the numbers\n",
    "    new_inr_vals = [round(float(''.join(value[:1] + '.' + value[1:])), 2) for value in new_inr_vals] #place a '.' after every 2 numbers, and convert to number to get the correct inr values\n",
    "    df.loc[df['Test_INR']>0, 'Test_INR'] = new_inr_vals\n",
    "    \n",
    "    return df\n",
    "\n",
    "#Label function that creates different datasets for each prediction target: Cancellation (yes/no), Paracetemol (yes/no), LOS (total time)\n",
    "def label_traces(df):\n",
    "    \n",
    "    df_can, df_par, df_los = df, df, df #create dataframe copies that can be manipulated\n",
    "    labels = ['Cancellation', 'Paracetemol', 'LOS'] #prediction targets\n",
    "    \n",
    "    for label in labels:\n",
    "        \n",
    "        #print(labels)\n",
    "        event_list, id_list, label_list = [], [], [] #create empty lists to store all trace_ids and trace_activites in\n",
    "        print('Now labeling with label: ', label)\n",
    "        \n",
    "        #when labeling the cancellation dataframe\n",
    "        if label == 'Cancellation':\n",
    "            for trace in df.groupby('TraceID'): #for each unique trace\n",
    "                events = list(trace[1].EventName)\n",
    "                event_list.append([''.join(x) for x in events])\n",
    "                id_list.append(list(trace[1].TraceID)[0])\n",
    "\n",
    "                if 'Cancellation' in events: #check if prediction target occurs in this trace or not)\n",
    "                    label_list.append(1)\n",
    "                else:\n",
    "                    label_list.append(0)\n",
    "\n",
    "            #make a df containing of each unique trace and the label\n",
    "            label_df = pd.DataFrame(list(zip(id_list, label_list)),\n",
    "                                   columns =['TraceID', 'Label'])\n",
    "\n",
    "            #merge the labels into the dataframe copy\n",
    "            df_can = df_can.merge(label_df, \n",
    "                                  on='TraceID', \n",
    "                                  how='left')\n",
    "            \n",
    "            #remove the rows with cancellation event from the dataset, which contains the answer to the label\n",
    "            df_can = df_can[df_can['EventName'] != 'Cancellation']\n",
    "\n",
    "        \n",
    "        #when labeling the paracetamol dataframe\n",
    "        elif label == 'Paracetemol':\n",
    "            for trace in df.groupby('TraceID'): #for each unique trace\n",
    "                events = list(trace[1].EventName)\n",
    "                event_list.append([''.join(x) for x in events])\n",
    "                id_list.append(list(trace[1].TraceID)[0])\n",
    "\n",
    "                if 'Paracetamol' in events: #check if prediction target occurs in this trace or not)\n",
    "                    label_list.append(1)\n",
    "                else:\n",
    "                    label_list.append(0)\n",
    "\n",
    "            #make a df containing of each unique trace and the label\n",
    "            label_df = pd.DataFrame(list(zip(id_list, label_list)),\n",
    "                                   columns =['TraceID', 'Label'])\n",
    "\n",
    "            #merge the labels into the dataframe copy\n",
    "            df_par = df_par.merge(label_df, \n",
    "                                  on='TraceID', \n",
    "                                  how='left')\n",
    "            \n",
    "            #remove the cancellation column from the dataset, which contains the answer to the label\n",
    "            df_par = df_par[df_par['EventName'] != 'Paracetamol']\n",
    "        \n",
    "        elif label == 'LOS':\n",
    "            los_calc_df = df[df['TraceID'].isin(list(df[df['EventName']== 'Admission']['TraceID']))] #filter the traces that don't include admission/discharge activity\n",
    "            los_calc_df = los_calc_df[los_calc_df['EventName'].isin(['Admission', 'Discharge'])] #filter only the admission/discharge activities\n",
    "\n",
    "            los_calc_df['Label'] = los_calc_df.groupby('TraceID')['Timestamp'].diff().dt.total_seconds() / 60 #calculate total length of stay (in minutes)\n",
    "            los_calc_df = los_calc_df[los_calc_df['EventName']=='Discharge'][['TraceID', 'Label']] #filter on only the rows/columns that include the total LOS\n",
    "\n",
    "            #merge the labels into the dataframe copy\n",
    "            df_los = df_los.merge(los_calc_df, \n",
    "                                  on='TraceID', \n",
    "                                  how='left')\n",
    "            \n",
    "            #filter traces with no prediction label (no admission/discharge activities) as well as ~15 significant outliers by only taking records below 2500\n",
    "            df_los = df_los.dropna(subset=['Label'])\n",
    "            df_los = df_los[df_los['Label']<2500]\n",
    "            df_los = df_los[df_los['Label']<500]\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            print('Unrecognized Label! Stopping function now')\n",
    "            return\n",
    "    \n",
    "    return df_can, df_par, df_los\n",
    "\n",
    "def export_and_save_traces(df, filename, folder_path = 'C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/'):\n",
    "    \n",
    "    #Save preprocessed data to new files, check if the file already exists or not\n",
    "    if os.path.exists(folder_path + filename + '.csv'):\n",
    "        overwrite = input('Warning, file already exist. Do you want to overwrite? type y/n: ') \n",
    "        \n",
    "        if overwrite.lower() == 'y': #check if you want to overwrite the files if they already exist\n",
    "            df.to_csv(path_or_buf = folder_path + filename + '.csv', sep=',', index=False)\n",
    "            df.to_excel(excel_writer = folder_path + filename + '.xlsx', index=False)\n",
    "            print('Files succesfully overwritten')\n",
    "        else:\n",
    "            print('Files not overwritten.')\n",
    "    else:\n",
    "        df.to_csv(path_or_buf = folder_path + filename + '.csv', sep=',', index=False)\n",
    "        df.to_excel(excel_writer = folder_path + filename + '.xlsx', index=False)\n",
    "        print('New preprocessed logs created: ', filename)\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c065fad3-f1ae-4035-87ff-a993675d5179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now labeling with label:  Cancellation\n",
      "Now labeling with label:  Paracetemol\n",
      "Now labeling with label:  LOS\n"
     ]
    }
   ],
   "source": [
    "df = preprocess_data(file='C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/newEventLog.xlsx')\n",
    "\n",
    "df_can, df_par, df_los = label_traces(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5cbbedc-6428-4abb-ab3d-12f56a2b45ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Warning, file already exist. Do you want to overwrite? type y/n:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files succesfully overwritten\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Warning, file already exist. Do you want to overwrite? type y/n:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files succesfully overwritten\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Warning, file already exist. Do you want to overwrite? type y/n:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files succesfully overwritten\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/'\n",
    "\n",
    "export_and_save_traces(df_can, 'EventLog_Processed_Cancel', folder_path)\n",
    "export_and_save_traces(df_par, 'EventLog_Processed_Paracetamol', folder_path)\n",
    "export_and_save_traces(df_los, 'EventLog_Processed_LOS', folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bdbfcc-8253-419f-a2f4-81f2ca03d46b",
   "metadata": {},
   "source": [
    "## One hot encoding\n",
    "\n",
    "One-hot encode the traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4400bcdd-0703-4491-a765-46abf06a349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple function that one hot encodes a column of a dataframe\n",
    "def one_hot_encoding(df):\n",
    "    #case id not encoded\n",
    "    for column in df.columns[1:]:\n",
    "        if not np.issubdtype(df[column], np.number):\n",
    "            # One hot encoding - eventual categorical nans will be ignored\n",
    "            one_hot = pd.get_dummies(df[column], prefix=column, prefix_sep='_')\n",
    "            #print(\"Encoded column: {} - Different keys: {}\".format(column, one_hot.shape[1]))\n",
    "            # Drop column as it is now encoded\n",
    "            df = df.drop(column, axis=1)\n",
    "            # Join the encoded df\n",
    "            df = df.join(one_hot)\n",
    "    #print(\"Categorical columns encoded\")\n",
    "    column_names = (one_hot.columns.tolist())\n",
    "    return df, column_names\n",
    "\n",
    "#function that takes the one-hot-encoded traces and combines them into a single one hot combined trace\n",
    "#duplicates (if events occur twice) are only registered once\n",
    "def combine_sequences(df):\n",
    "    data = [] #list to store all one-hot encoded traces\n",
    "    trace = [] #list to store data on a single trace, which will be updated when each activity is added\n",
    "    caseID = df.iloc[0][0] #the first TraceID\n",
    "    trace.append(df.iloc[0][1:].tolist())\n",
    "    for index, line in df.iloc[1:, :].iterrows(): #for all rows in the df\n",
    "        case = line[0]\n",
    "        if case == caseID:\n",
    "            trace[0] = [x+y if x+y<2 else x for x, y in zip(trace[0], line.iloc[1:].tolist())] #add the one-hot encoding to trace[0] through sum, if the event already occured, keep the 1\n",
    "        else:\n",
    "            caseID = case #set new case\n",
    "            data.append(trace[:len(trace)][0]) #add one-hot encoded trace to data\n",
    "            trace = []\n",
    "            trace.append(line.iloc[1:].tolist())\n",
    "    # last case\n",
    "    data.append(trace[:len(trace)][0])\n",
    "    return data\n",
    "\n",
    "\n",
    "def one_hot_and_combine(df, additional=False):\n",
    "    \n",
    "    #one-hot encode activities\n",
    "    df = df.sort_values(by=['TraceID', 'Timestamp']).reset_index().drop(columns='index') #load data\n",
    "    labels = df.groupby('TraceID').mean()['Label'].tolist()#.astype(int) #save labels so they can be added later\n",
    "    \n",
    "    one_hot_df = df[['TraceID', 'EventName']] #keep only relevant columns\n",
    "    one_hot_df, column_names = one_hot_encoding(one_hot_df) #one hot encode every trace per event (each row = 1 event)\n",
    "    data = combine_sequences(one_hot_df) #aggregate the one hot encoded events to a single trace\n",
    "    one_hot_df = pd.DataFrame(data, columns=column_names) #make df of encoded data\n",
    "    one_hot_df['Label'] = labels #add labels back\n",
    "    \n",
    "    #one-hot encode additional features and add to dataframe if required, also add numerical features\n",
    "    if additional:\n",
    "        one_hot_df_additional = df[['TraceID', 'MedicationCode']]\n",
    "        one_hot_df_additional, column_names_additional = one_hot_encoding(one_hot_df_additional)\n",
    "        data_additional = combine_sequences(one_hot_df_additional) \n",
    "        one_hot_df_additional = pd.DataFrame(data_additional, columns=column_names_additional)\n",
    "        \n",
    "        #merge with other df on index and also add numerical additional features\n",
    "        additional_numerical_df = df.groupby('TraceID').mean().reset_index().drop(columns=['Label', 'TraceID']) #get numerical features\n",
    "        one_hot_df = one_hot_df.merge(one_hot_df_additional, left_index=True, right_index=True) #first merge with one-hot additional features\n",
    "        one_hot_df = one_hot_df.merge(additional_numerical_df, left_index=True, right_index=True) #then merge with numerical features\n",
    "        \n",
    "    \n",
    "    return one_hot_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7c84201e-12f2-4bcf-9b3e-5cf66c5172d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read excel file\n",
    "processed_df_can = pd.read_excel('C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/EventLog_Processed_Cancel.xlsx')\n",
    "processed_df_par = pd.read_excel('C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/EventLog_Processed_Paracetamol.xlsx')\n",
    "processed_df_los = pd.read_excel('C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/EventLog_Processed_LOS.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c8af65e5-7256-4433-9b29-185437044791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Warning, file already exist. Do you want to overwrite? type y/n:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files succesfully overwritten\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Warning, file already exist. Do you want to overwrite? type y/n:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files succesfully overwritten\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Warning, file already exist. Do you want to overwrite? type y/n:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files succesfully overwritten\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Warning, file already exist. Do you want to overwrite? type y/n:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files succesfully overwritten\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Warning, file already exist. Do you want to overwrite? type y/n:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files succesfully overwritten\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Warning, file already exist. Do you want to overwrite? type y/n:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files succesfully overwritten\n"
     ]
    }
   ],
   "source": [
    "#one hot encode data\n",
    "one_hot_can= one_hot_and_combine(processed_df_can, additional=False)\n",
    "one_hot_par= one_hot_and_combine(processed_df_par, additional=False)\n",
    "one_hot_los= one_hot_and_combine(processed_df_los, additional=False)\n",
    "\n",
    "#one hot encode data with additional data\n",
    "one_hot_can_additional = one_hot_and_combine(processed_df_can, additional=True)\n",
    "one_hot_par_additional = one_hot_and_combine(processed_df_par, additional=True)\n",
    "one_hot_los_additional = one_hot_and_combine(processed_df_los, additional=True)\n",
    "\n",
    "#save results\n",
    "one_hot_folder_path = 'C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/encoded_logs/one_hot_encoded_logs/'\n",
    "export_and_save_traces(one_hot_can, 'one_hot_can', one_hot_folder_path)\n",
    "export_and_save_traces(one_hot_par, 'one_hot_par', one_hot_folder_path)\n",
    "export_and_save_traces(one_hot_los, 'one_hot_los', one_hot_folder_path)  \n",
    "export_and_save_traces(one_hot_can_additional, 'one_hot_can_additional', one_hot_folder_path)\n",
    "export_and_save_traces(one_hot_par_additional, 'one_hot_par_additional', one_hot_folder_path)\n",
    "export_and_save_traces(one_hot_los_additional, 'one_hot_los_additional', one_hot_folder_path)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92914a1-336b-49fc-9876-6a65af71ea90",
   "metadata": {},
   "source": [
    "## Additional data split\n",
    "\n",
    "A seperate split is made that just keeps the additional data columns. This is for a an experiment to see how the models perform in a more traditional ML approach without use of any trace event data.\n",
    "\n",
    "To achieve this, I simply take the already one-hot encoded data and remove the trace columns, so I don't have to regroup the data and re-add the label. I save the additional features seperately using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6679f80-97cc-405c-bb2a-7bc8cd11237e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Warning, file already exist. Do you want to overwrite? type y/n:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files succesfully overwritten\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Warning, file already exist. Do you want to overwrite? type y/n:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files succesfully overwritten\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Warning, file already exist. Do you want to overwrite? type y/n:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files succesfully overwritten\n"
     ]
    }
   ],
   "source": [
    "#load the previously one-hot-encoded \n",
    "additional_can = pd.read_csv('C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/encoded_logs/one_hot_encoded_logs/one_hot_can_additional.csv')\n",
    "additional_par = pd.read_csv('C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/encoded_logs/one_hot_encoded_logs/one_hot_par_additional.csv')\n",
    "additional_los = pd.read_csv('C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/encoded_logs/one_hot_encoded_logs/one_hot_los_additional.csv')\n",
    "\n",
    "#remove the trace event columns\n",
    "additional_can = additional_can[[column for column in additional_can if 'EventName' not in column]]\n",
    "additional_par = additional_par[[column for column in additional_par if 'EventName' not in column]]\n",
    "additional_los = additional_los[[column for column in additional_los if 'EventName' not in column]]\n",
    "\n",
    "#Save the dfs filled with solely the additional data\n",
    "agg_folder_path = 'C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/encoded_logs/additional_data/'\n",
    "export_and_save_traces(additional_can, 'additional_can', agg_folder_path)\n",
    "export_and_save_traces(additional_par, 'additional_par', agg_folder_path)\n",
    "export_and_save_traces(additional_los, 'additional_los', agg_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2404652a-c616-4fc7-a753-cefb623772e6",
   "metadata": {},
   "source": [
    "## Word2Vec embbeding\n",
    "\n",
    "Adapated from: https://github.com/gbrltv/business_process_encoding/blob/master/compute_encoding/word2vec_cbow.py\n",
    "\n",
    "The Gensim word2vec model uses a list of lists as input, where each list is filled with all activities in a trace. Therefore, each trace is represented by a list that includes all activities that have happened in that particular trace. Each of these lists is embedded into an embedding the size of the vector_size variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f181fe7e-8943-4df3-bae4-6b10baa04f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to read an event log. Parameters: file path, file name, replace \"space\" with \"-\"\n",
    "def read_log(path, log, replace_space: str = '-'):\n",
    "    df_raw = pd.read_csv(f'{path}/{log}')\n",
    "    df_raw['EventName'] = df_raw['EventName'].str.replace(' ', replace_space)\n",
    "    df_proc = df_raw[['TraceID', 'EventName', 'Label']]\n",
    "\n",
    "    return df_proc, df_raw\n",
    "\n",
    "#Function that creates a txt model. Paremeters: text-based model containing encodings, list of cases treated as sentences by the model\n",
    "def train_text_model(model, cases):\n",
    "    model.build_vocab(cases)\n",
    "    model.train(cases, total_examples=len(cases), epochs=10)\n",
    "\n",
    "    return model\n",
    "\n",
    "#Function that creates a list of cases for model training.\n",
    "def retrieve_traces(df):\n",
    "    traces, y, ids = [], [], []\n",
    "    for group in df.groupby('TraceID'):\n",
    "        events = list(group[1].EventName)\n",
    "        traces.append([''.join(x) for x in events])\n",
    "        y.append(list(group[1].Label)[0])\n",
    "        ids.append(list(group[1].TraceID)[0])\n",
    "\n",
    "    return ids, traces, y\n",
    "\n",
    "#The Gensim Word2Vec model requires a list of lists, where each list contains a list of all activities in 1 trace. \n",
    "#This function converts the raw input to that mapping\n",
    "def convert_traces_mapping(traces_raw, mapping):\n",
    "    traces = []\n",
    "    for trace in traces_raw:\n",
    "        current_trace = []\n",
    "        for act in trace:\n",
    "            current_trace.append(mapping[act])\n",
    "        traces.append(current_trace)\n",
    "        \n",
    "    return traces\n",
    "\n",
    "#Function that orders a list alphanumerically\n",
    "def sort_alphanumeric(data):\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "    \n",
    "    return sorted(data, key=alphanum_key)\n",
    "\n",
    "#Function that computes the average/max feature vector for each trace. Paremeters: text-based model containing encodings and list of traces treated as sequences\n",
    "def average_feature_vector(model, traces):\n",
    "    vectors_average, vectors_max = [], []\n",
    "    for trace in traces: # for each trace\n",
    "        trace_vector = []\n",
    "        for token in trace: # for each activity in the trace\n",
    "            try:\n",
    "                trace_vector.append(model.wv[token])# get numpy vector of the activity (token)\n",
    "            except KeyError:\n",
    "                pass\n",
    "        vectors_average.append(np.array(trace_vector).mean(axis=0)) #aggregate all embedding values per activity into n dimensions\n",
    "        vectors_max.append(np.array(trace_vector).max(axis=0)) #aggregate again, but this time with max value instead of mean\n",
    "\n",
    "    return vectors_average, vectors_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b6f79a1-5097-48dd-8621-86bf84707c34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now encoding file:  EventLog_Processed_Cancel.csv\n",
      "Now encoding file:  EventLog_Processed_LOS.csv\n",
      "Now encoding file:  EventLog_Processed_Paracetamol.csv\n",
      "Embedding the traces using Word2Vec took: 0.03 mins\n"
     ]
    }
   ],
   "source": [
    "#define paths and embedding dimensions, also note time\n",
    "t = time()\n",
    "\n",
    "dimension = 8 #embedding dimension, 8 showed best performance in early experimentation\n",
    "csv_path =  'C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/' #path where the preprocessed csv files are stored\n",
    "save_path = 'C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/encoded_logs/word2vec' #path where results are saved\n",
    "csv_list = ['EventLog_Processed_Cancel.csv', 'EventLog_Processed_LOS.csv', 'EventLog_Processed_Paracetamol.csv'] #file names for csv files\n",
    "additional_csv_locs = ['C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/encoded_logs/additional_data/additional_can.csv',\n",
    "                       'C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/encoded_logs/additional_data/additional_los.csv',\n",
    "                       'C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/encoded_logs/additional_data/additional_par.csv'] #file paths to additional data \n",
    "\n",
    "#make directories to store results in\n",
    "for type in ['average']:\n",
    "    os.makedirs(f'{save_path}/{type}/{dimension}', exist_ok=True)\n",
    "\n",
    "#for all csv files that need to be encoded\n",
    "for file, additional_file in zip(csv_list, additional_csv_locs):\n",
    "    print('Now encoding file: ', file)\n",
    "    \n",
    "    # read event log and import case id, traces and labels\n",
    "    df_proc, df_raw = read_log(csv_path, file) # create two dataframes, one with and one without additional data\n",
    "    ids, traces, y = retrieve_traces(df_proc) # retrieve the traces from the non-additional data dataframe\n",
    "    \n",
    "    # generate model\n",
    "    model = Word2Vec(vector_size=dimension, window=3, min_count=1, sg=0, workers=-1)\n",
    "    model = train_text_model(model, traces)\n",
    "\n",
    "    # calculating the feature vector for each sentence (trace)\n",
    "    vectors_average, vectors_max = average_feature_vector(model, traces)\n",
    "\n",
    "    # generate file names including and excluding additional data\n",
    "    filename_avg = file.split('.')[0] + '_' + str(dimension) + '_word2vec_avg.csv' \n",
    "    filename_additional_avg = file.split('.')[0] + '_' + str(dimension) + '_word2vec_avg_additional.csv'\n",
    "    \n",
    "    #generate the output df, also add back traceIDs and labels\n",
    "    out_df = pd.DataFrame(vectors_average, columns=[f'feature_{i}' for i in range(dimension)]) #add feature prefix\n",
    "    out_df['TraceID'] = ids\n",
    "    out_df['Label'] = y\n",
    "    \n",
    "    #create version including additional data\n",
    "    additional_df = pd.read_csv(additional_file)\n",
    "    additional_df = additional_df.drop('Label', axis=1)#drop redundant 2nd label\n",
    "    out_df_additional = pd.concat([out_df, additional_df], axis=1) #axis = 1 since we want to paste columns next to each other instead of rows\n",
    "\n",
    "    #save results\n",
    "    out_df.to_csv(f'{save_path}/average/{dimension}/{filename_avg}', index=False)\n",
    "    out_df_additional.to_csv(f'{save_path}/average/{dimension}/{filename_additional_avg}', index=False)\n",
    "\n",
    "print('Embedding the traces using Word2Vec took: {} mins'.format(round((time() - t) / 60, 2)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc53ef7-7d17-4fe1-858b-c695184d984b",
   "metadata": {},
   "source": [
    "## Aggregation encoding\n",
    "\n",
    "This code calculates the aggregation encoding\n",
    "\n",
    "In short:\n",
    "- create seperate aggregated measures for all numeric values (min, max, mean, sum, std)\n",
    "- measure frequencies of categorical values (activity/resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3350353e-ed97-424c-aa61-d4d25336d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_encoder(df, skip_columns):\n",
    "    \n",
    "    numeric_cols = [] #list to store numerical column names in\n",
    "    cat_cols = [] #list to store categorical column names in\n",
    "    agg_measures_dict = {} #dict that will be updated with new columns (trace_id - new column value pairs). Will be used to make aggregated df\n",
    "    \n",
    "    agg_df = df.groupby('TraceID')['TraceID']\n",
    "    \n",
    "    #determine numeric columns\n",
    "    for column in df.columns:\n",
    "        if column not in skip_columns: #check if column does not need to be encoded\n",
    "            if is_numeric_dtype(df[column]):\n",
    "                numeric_cols.append(column)\n",
    "            else:\n",
    "                cat_cols.append(column)\n",
    "    \n",
    "    for col in numeric_cols: #calculate aggregate measures for numeric values and add them to the dictionary\n",
    "        agg_measures_dict.update(\n",
    "            df.groupby('TraceID').agg(\n",
    "                max_value=(col, max),\n",
    "                min_value=(col, min),\n",
    "                sum_value=(col, sum),\n",
    "                mean_value=(col, np.mean),\n",
    "                std_value=(col, np.std)\n",
    "            ).to_dict()\n",
    "        )\n",
    "\n",
    "        #rename the keys to include the original column name\n",
    "        agg_measures_dict['max_{}'.format(col)] = agg_measures_dict.pop('max_value')\n",
    "        agg_measures_dict['min_{}'.format(col)] = agg_measures_dict.pop('min_value')\n",
    "        agg_measures_dict['sum_{}'.format(col)] = agg_measures_dict.pop('sum_value')\n",
    "        agg_measures_dict['mean_{}'.format(col)] = agg_measures_dict.pop('mean_value')\n",
    "        agg_measures_dict['std_{}'.format(col)] = agg_measures_dict.pop('std_value')\n",
    "        \n",
    "    #make dataframe of the dictionaries and re-add the skip columns (trace id, timestamp, label)\n",
    "    agg_df = pd.DataFrame.from_dict(agg_measures_dict).reset_index().rename(columns={'index': 'TraceID'})\n",
    "    \n",
    "    \n",
    "    #for categorical columns, calculate frequency per value (activity, medicationcode) in trace\n",
    "    agg_df = agg_df.merge(df.groupby(['TraceID', 'EventName'])['Label'].count().unstack().add_prefix('freq_'),\n",
    "                          on = 'TraceID',\n",
    "                          how = 'left')\n",
    "   \n",
    "    # Also calculate frequency of medicationcode per trace id, create this in a seperate df since its considered as additional data\n",
    "    agg_df_additional = agg_df.merge(df.groupby(['TraceID', 'MedicationCode'])['Label'].count().unstack().add_prefix('freq_'),\n",
    "                          on = 'TraceID',\n",
    "                          how = 'left')\n",
    "\n",
    "\n",
    "    #finally re-add the label column to both df's from the skipped columns, timestamp is not added since the data has been aggregated\n",
    "    agg_df = agg_df.merge(df[['TraceID', 'Label']].groupby('TraceID').max(), \n",
    "                          on='TraceID', \n",
    "                          how='left') \n",
    "    \n",
    "    agg_df_additional = agg_df_additional.merge(df[['TraceID', 'Label']].groupby('TraceID').max(), \n",
    "                          on='TraceID', \n",
    "                          how='left') \n",
    "\n",
    "    return agg_df, agg_df_additional\n",
    "                      \n",
    "\n",
    "def split_agg(df):\n",
    "    \n",
    "    skip_columns = []\n",
    "    skip_terms = ['max', 'min', 'sum', 'mean', 'std']\n",
    "    \n",
    "    for term in skip_terms:\n",
    "        skip_columns.append([col for col in df.columns if term in col])\n",
    "    \n",
    "    skip_columns = ([col for sublist in skip_columns for col in sublist])\n",
    "    \n",
    "    return df[[col for col in df.columns if col not in skip_columns]]\n",
    "    #return df[df.columns not in [col for sublist in skip_columns for col in sublist]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "909f9414-1046-4eff-acd6-a4dec79834da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Warning, file already exist. Do you want to overwrite? type y/n:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files succesfully overwritten\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Warning, file already exist. Do you want to overwrite? type y/n:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files succesfully overwritten\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Warning, file already exist. Do you want to overwrite? type y/n:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files succesfully overwritten\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Warning, file already exist. Do you want to overwrite? type y/n:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files succesfully overwritten\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Warning, file already exist. Do you want to overwrite? type y/n:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files succesfully overwritten\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Warning, file already exist. Do you want to overwrite? type y/n:  y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files succesfully overwritten\n"
     ]
    }
   ],
   "source": [
    "#create two versions, one with and one without inclusion of additional data\n",
    "agg_df_can, agg_df_can_additional = aggregate_encoder(processed_df_can, ['TraceID', 'Timestamp', 'Label'])\n",
    "agg_df_can = split_agg(agg_df_can)#.columns\n",
    "\n",
    "agg_df_par, agg_df_par_additional = aggregate_encoder(processed_df_par, ['TraceID', 'Timestamp', 'Label'])\n",
    "agg_df_par = split_agg(agg_df_par)#.columns\n",
    "\n",
    "agg_df_los, agg_df_los_additional = aggregate_encoder(processed_df_los, ['TraceID', 'Timestamp', 'Label'])\n",
    "agg_df_los = split_agg(agg_df_los)#.columns\n",
    "\n",
    "#Save the agg dataframes\n",
    "agg_folder_path = 'C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/encoded_logs/aggregated/'\n",
    "\n",
    "export_and_save_traces(agg_df_can, 'aggregated_can', agg_folder_path)\n",
    "export_and_save_traces(agg_df_can_additional, 'aggregated_can_additional', agg_folder_path)\n",
    "\n",
    "export_and_save_traces(agg_df_par, 'aggregated_par', agg_folder_path)\n",
    "export_and_save_traces(agg_df_par_additional, 'aggregated_par_additional', agg_folder_path)\n",
    "\n",
    "export_and_save_traces(agg_df_los, 'aggregated_los', agg_folder_path)\n",
    "export_and_save_traces(agg_df_los_additional, 'aggregated_los_additional', agg_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec33aa44-2aa9-4c4c-9f1b-5bcb1835fad1",
   "metadata": {},
   "source": [
    "## autoencoder\n",
    "Since the aggregated encodings are highly dimensional, an autoencoder is used to significantly reduce dimensionality in the data. The result of this autoencoder will be passed to the prediction models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "569b830d-4d54-4c23-80bb-af52890cd202",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "np.random.seed(42)\n",
    "#from tensorflow import set_random_seed\n",
    "#set_random_seed(seed)\n",
    "\n",
    "def autoencoder(file_location):\n",
    "    model_name = file_location.split(\"\\\\\")[-1:][0].split('.')[0] #get filename (without.csv)\n",
    "    new_model_name = 'ae_agg_' + '_'.join(model_name.split('_')[1:])\n",
    "    output_dir = 'C:\\\\Users\\\\20190337\\\\Downloads\\Tracebook_v2 (Projectfolder)\\\\encoded_logs\\\\ae_agg\\\\'\n",
    "    \n",
    "    print('Now autoencoding: ', model_name, '...')\n",
    "    \n",
    "    agg_df = pd.read_csv(file_location)\n",
    "    agg_df = agg_df.fillna(0)\n",
    "\n",
    "    agg_X = agg_df[agg_df.columns.difference(['TraceID', 'Label'])]\n",
    "    agg_y = agg_df['Label']\n",
    "\n",
    "    input_len = len(agg_X.columns)\n",
    "\n",
    "    #train test val split\n",
    "    if 'los' in model_name:\n",
    "        x_train, x_test, y_train, y_test = train_test_split(agg_X, agg_y, test_size = 0.2, random_state = 42, shuffle = True)\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2, random_state = 42, shuffle = True)\n",
    "    else:\n",
    "        x_train, x_test, y_train, y_test = train_test_split(agg_X, agg_y, test_size = 0.2, random_state = 42, shuffle = True, stratify = agg_y)\n",
    "        x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2, random_state = 42, shuffle = True, stratify = y_train)\n",
    "\n",
    "    x_train = x_train.values.astype(float)\n",
    "    x_val = x_val.values.astype(float)\n",
    "    x_test = x_test.values.astype(float)\n",
    "\n",
    "    #input layer\n",
    "    input_layer = Input(shape=(input_len,))\n",
    "\n",
    "    #encoder\n",
    "    encoded = Dense(units=input_len, activation='tanh')(input_layer)\n",
    "    encoded = Dense(units=(round(input_len*0.667)), activation='tanh')(encoded) #reduce by 1/3\n",
    "    encoded = Dense(units=(round(input_len*0.445)), activation='tanh')(encoded) #reduce by another 1/3\n",
    "    encoded = Dense(units=(round(input_len*0.297)), activation='tanh')(encoded) #reduce by another 1/3\n",
    "\n",
    "    #latent-space\n",
    "    encoded = Dense(units=(round(input_len*0.198)), activation='tanh')(encoded) # reduce once more, ending up at around 1/5th of original size\n",
    "\n",
    "    #decoder\n",
    "    decoded = Dense(units=(round(input_len*0.297)), activation='tanh')(encoded) #increase by 1/3\n",
    "    decoded = Dense(units=(round(input_len*0.445)), activation='tanh')(decoded) #increase by another 1/3\n",
    "    decoded = Dense(units=(round(input_len*0.667)), activation='tanh')(decoded) #increase by another 1/3\n",
    "    decoded = Dense(units=input_len, activation='tanh')(decoded) #return to original size\n",
    "\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    encoder = Model(input_layer, encoded)\n",
    "\n",
    "    autoencoder.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "    autoencoder.fit(x_train, x_train,\n",
    "                    epochs=50,\n",
    "                    batch_size=128,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(x_val, x_val),\n",
    "                    verbose=0)\n",
    "\n",
    "    #encoder.save(\"dataset/\"+log+\"/\"+log+\"_encoder.h5\")\n",
    "    encoded_agg = encoder.predict(agg_X)\n",
    "\n",
    "    scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler.fit(encoded_agg)\n",
    "    \n",
    "    encoded_agg_norm = scaler.transform(encoded_agg)\n",
    "    encoded_agg_norm = pd.DataFrame(encoded_agg_norm)\n",
    "    encoded_agg_norm.insert(len(encoded_agg_norm.columns), 'Label', agg_y)\n",
    "    \n",
    "    #save new df\n",
    "    encoded_agg_norm.to_csv(output_dir + new_model_name + '.csv', index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1d7a8d47-0532-4c42-9782-0bd5b14fc7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now autoencoding:  aggregated_can ...\n",
      "Now autoencoding:  aggregated_can_additional ...\n",
      "Now autoencoding:  aggregated_los ...\n",
      "Now autoencoding:  aggregated_los_additional ...\n",
      "Now autoencoding:  aggregated_par ...\n",
      "Now autoencoding:  aggregated_par_additional ...\n"
     ]
    }
   ],
   "source": [
    "def csv_file_list():\n",
    "    # make a list of all csv files\n",
    "    results_folder = 'C:\\\\Users\\\\20190337\\\\Downloads\\Tracebook_v2 (Projectfolder)\\\\encoded_logs\\\\aggregated\\\\'\n",
    "    file_extension = '*.csv' #define file extension\n",
    "    all_csv_files = [] #store csv files in this list\n",
    "    for path, subdir, files in os.walk(results_folder):\n",
    "        for file in glob(os.path.join(path, file_extension)):\n",
    "            all_csv_files.append(file)\n",
    "            \n",
    "    return all_csv_files\n",
    "\n",
    "agg_csv_files= csv_file_list()\n",
    "for file in agg_csv_files:\n",
    "    autoencoder(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bd3a12-c4b6-46aa-8ef4-8ac7b86030d9",
   "metadata": {},
   "source": [
    "## Tokenized encoding\n",
    "This part of the code uses a custom adaption of the functions that were originally created by Bukhsh et al. (https://github.com/Zaharah/processtransformer/tree/178f4c0bde5efb6bb25834d3493841482ad58227). New functions have been added to their python package that allow for different prediction tasks: namely outcome prediction in my case. The adapted functions are stored in the transformer_lib folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5b05fc6b-8619-41ae-a0a7-02e041cf5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_traces(data_loader):\n",
    "    #prepare train/test data\n",
    "    (train_df, test_df, full_df, x_word_dict, y_word_dict, max_case_length, \n",
    "            vocab_size, num_output) = data_loader.load_data('outcome')\n",
    "\n",
    "    # Prepare training and test examples for outcome prediction task\n",
    "    token_full_x = data_loader.prepare_data_outcome(full_df, x_word_dict, max_case_length) #tokenizes all data\n",
    "\n",
    "    # create new dataframe from the now tokenized traces\n",
    "    tokenized_full_df = pd.concat([full_df.reset_index(drop=True),\n",
    "                              pd.DataFrame(token_full_x).add_prefix('token_').reset_index(drop=True)], \n",
    "                              axis=1).drop(columns='activities')\n",
    "\n",
    "    #put outcome at end of the dataframe for continuity with other datasets\n",
    "    cols = list(tokenized_full_df.columns.values) #Make list of all columns, same for train/test so only have to do it once\n",
    "    cols.pop(cols.index('outcome')) #Remove outcome\n",
    "    tokenized_full_df = tokenized_full_df[cols+['outcome']]\n",
    "\n",
    "    return tokenized_full_df\n",
    "    \n",
    "def prepare_tokenize(name, file_location, dir_path, additional_df_loc):\n",
    "\n",
    "    #Initialize data_processor for cancellation dataset\n",
    "    data_processor = LogsDataProcessor(name=name, \n",
    "        filepath=file_location, \n",
    "        columns = ['TraceID', 'EventName', 'Timestamp', 'Label'],\n",
    "        dir_path= dir_path, \n",
    "        pool = 1) #changed from 4 to 1, number of CPU's to use in processing\n",
    "\n",
    "    #process the data\n",
    "    data_processor.process_logs(task='OUTCOME', sort_temporally=False)\n",
    "\n",
    "    #load the data\n",
    "    data_loader = LogsDataLoader(name=name)\n",
    "\n",
    "    #tokenize the data\n",
    "    token_full = tokenize_traces(data_loader)\n",
    "\n",
    "    #create seperate dataframe with additional data        \n",
    "    additional_df = pd.read_csv(additional_df_loc)\n",
    "    additional_df = additional_df.drop('Label', axis=1)#drop redundant 2nd label\n",
    "    token_full_agg = pd.concat([token_full, additional_df], axis=1) #axis = 1 since we want to paste columns next to each other instead of rows\n",
    "\n",
    "    return token_full, token_full_agg\n",
    "\n",
    "#simple function to save tokenized results, input 4 dataframe and directory information\n",
    "def save_tokenized(token_full, token_full_agg, dir_path, foldername, filename):\n",
    "    token_full.to_csv(dir_path + foldername + '/processed/' + filename + '_full.csv', index = False)\n",
    "    token_full_agg.to_csv(dir_path + foldername + '/processed/' + filename + '_full_additional.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbfb79a-99d7-4663-a1fe-f4eaf2f41173",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = 'C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/encoded_logs/transformer/'\n",
    "additional_csv_locs = ['C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/encoded_logs/additional_data/additional_can.csv',\n",
    "                       'C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/encoded_logs/additional_data/additional_los.csv',\n",
    "                       'C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/encoded_logs/additional_data/additional_par.csv'] \n",
    "\n",
    "#tokenize all three dataset prediction targets\n",
    "token_full_can, token_full_can_agg = prepare_tokenize(name='processed_df_can', \n",
    "                                                      file_location='C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/EventLog_Processed_Cancel.csv',\n",
    "                                                      dir_path=dir_path, additional_df_loc=additional_csv_locs[0])\n",
    "\n",
    "token_full_par, token_full_par_agg = prepare_tokenize(name='processed_df_par', \n",
    "                                                    file_location='C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/EventLog_Processed_Paracetamol.csv',\n",
    "                                                    dir_path=dir_path, additional_df_loc=additional_csv_locs[2])\n",
    "\n",
    "token_full_los, token_full_los_agg = prepare_tokenize(name='processed_df_los', \n",
    "                                                      file_location='C:/Users/20190337/Downloads/Tracebook_v2 (Projectfolder)/EventLog_Processed_LOS.csv',\n",
    "                                                      dir_path=dir_path, additional_df_loc=additional_csv_locs[1])\n",
    "\n",
    "\n",
    "#save results\n",
    "save_tokenized(token_full_can, token_full_can_agg, dir_path, foldername='processed_df_can', filename='tokenized_can')\n",
    "save_tokenized(token_full_par, token_full_par_agg, dir_path, foldername='processed_df_par', filename='tokenized_par')\n",
    "save_tokenized(token_full_los, token_full_los_agg, dir_path, foldername='processed_df_los', filename='tokenized_los')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
